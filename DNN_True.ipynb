{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ディープニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリを読み込む\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ScratchDeepNeuralNetrowkClassifierのfitメソッド内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.sigma : ガウス分布の標準偏差\n",
    "# self.lr : 学習率\n",
    "# self.n_nodes1 : 1層目のノード数\n",
    "# self.n_nodes2 : 2層目のノード数\n",
    "# self.n_output : 出力層のノード数\n",
    "\n",
    "# optimizer = SGD(self.lr)\n",
    "# self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "# self.activation1 = Tanh()\n",
    "# self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "# self.activation2 = Tanh()\n",
    "# self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "# self.activation3 = Softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- イテレーションごとのフォワード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1 = self.FC1.forward(X)\n",
    "# Z1 = self.activation1.forward(A1)\n",
    "# A2 = self.FC2.forward(Z1)\n",
    "# Z2 = self.activation2.forward(A2)\n",
    "# A3 = self.FC3.forward(Z2)\n",
    "# Z3 = self.activation3.forward(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- イテレーションごとのバックワード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "# dZ2 = self.FC3.backward(dA3)\n",
    "# dA2 = self.activation2.backward(dZ2)\n",
    "# dZ1 = self.FC2.backward(dA2)\n",
    "# dA1 = self.activation1.backward(dZ1)\n",
    "# dZ0 = self.FC1.backward(dA1) # dZ0は使用しない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】全結合層のクラス化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全結合層のクラス化を行なってください。\n",
    "\n",
    "\n",
    "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    "\n",
    "\n",
    "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    "\n",
    "\n",
    "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、すべて全結合層が持つインスタンス変数にすることができます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        # 初期化\n",
    "        self.optimizer = optimizer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        #self.initializer = initializer\n",
    "        \n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = initializer.B(self.n_nodes2)\n",
    "        \n",
    "        # 勾配更新の際に使用（AdaGradのみ）\n",
    "        self.HW = 0\n",
    "        self.HB = 0\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        # 逆伝播時に使用\n",
    "        self.Z = X\n",
    "        # 順伝播の計算部分の本体\n",
    "      \n",
    "        self.A = X @ self.W + self.B\n",
    "        pass\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # バイアス項の勾配\n",
    "        self.dB = np.sum(dA,axis=0)\n",
    "        # バイアス項以外の勾配\n",
    "        self.dW =self.Z.T@dA\n",
    "        # 逆伝播させる値\n",
    "        self.dZ =dA@self.W.T\n",
    "        # 重み更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return self.dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】初期化方法のクラス化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期化を行うコードをクラス化してください。\n",
    "\n",
    "\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        pass\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        pass\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】最適化手法のクラス化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適化手法のクラス化を行なってください。\n",
    "\n",
    "\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # \n",
    "        layer.W -= self.lr * layer.dW / len(layer.Z)\n",
    "        layer.B -= self.lr * layer.dB / len(layer.Z)\n",
    "        return layer      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】活性化関数のクラス化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数のクラス化を行なってください。\n",
    "\n",
    "\n",
    "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    \"\"\"シグモイド\"\"\"\n",
    "    def forward(self,A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        --------\n",
    "        A : 順伝播されてきた値\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        Z = 1 / (1 + np.exp(-self.A))\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        --------\n",
    "        dZ : 逆伝播されてきた値\n",
    "        \"\"\"\n",
    "        dA = dZ * ((1 / ( 1 + np.exp(-sself.a))) - (1 / ( 1 + np.exp(-self.A)))**2)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"tanh\"\"\"\n",
    "    def forward(self,A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        --------\n",
    "        A : 順伝播されてきた値\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        Z = np.tanh(self.A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        --------\n",
    "        dZ : 逆伝播されてきた値\n",
    "        \"\"\"\n",
    "        dA = dZ * ( 1 - np.tanh(self.A)**2)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"softmax\"\"\"\n",
    "    def forward(self,A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        --------\n",
    "        A : 順伝播されてきた値\n",
    "        \"\"\"\n",
    "        Z = np.exp(A) / np.sum(np.exp(A),axis=1).reshape(-1,1)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,Z,y):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        --------\n",
    "        Z : 出力値\n",
    "        y : 正解データ\n",
    "        \"\"\"\n",
    "        # 逆伝播の値\n",
    "        dA = Z - y\n",
    "        # 損失\n",
    "        loss = - np.sum(y * np.log(Z)) / len(y)\n",
    "        return dA,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】ReLUクラスの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
    "\n",
    "\n",
    "ReLUは以下の数式です。\n",
    "\n",
    "$$\n",
    "f(x) = ReLU(x) = (x) if x>0,(0) if x≦0.\n",
    "$$\n",
    "\n",
    "$x$ : ある特徴量。スカラー\n",
    "\n",
    "\n",
    "実装上はnp.maximumを使い配列に対してまとめて計算が可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一方、バックプロパゲーションのための $x$ に関する $f(x)$ の微分は以下のようになります。\n",
    "\n",
    "$$\n",
    "∂f(x)∂x= 1if x>0,0if x≦0.\n",
    "$$\n",
    "\n",
    "数学的には微分可能ではないですが、 $x=0$ のとき $0$ とすることで対応しています。\n",
    "\n",
    "\n",
    "フォワード時の $x$ の正負により、勾配を逆伝播するかどうかが決まるということになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"relu\"\"\"\n",
    "    def forward(self,A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        --------\n",
    "        A : 順伝播されてきた値\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        Z = np.maximum(0,A) # maximum関数=引数同士を比べて大きい値が返ってくる\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        --------\n",
    "        dZ : 逆伝播されてきた値\n",
    "        \"\"\"\n",
    "        # self.Aが0より大きければ、１をそれ以外の時は０を返す\n",
    "        dA = dZ * np.where(self.A > 0,1,0)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】重みの初期値"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
    "\n",
    "\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavierの初期値"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xavierの初期値における標準偏差 $\\sigma$ は次の式で求められます。\n",
    "\n",
    "$$\n",
    "σ=1√n\n",
    "$$\n",
    "$n$ : 前の層のノード数\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heの初期値"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heの初期値における標準偏差 $\\sigma$ は次の式で求められます。\n",
    "\n",
    "$$\n",
    "σ=√2n\n",
    "$$\n",
    "\n",
    "$n$ : 前の層のノード数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"Xavier\"\"\"\n",
    "    def __init___(self,sigma):\n",
    "        _ = sigma\n",
    "        \n",
    "    def W(self,n_nodes1,n_nodes2):\n",
    "        \"\"\"\n",
    "        コンストラクタ\n",
    "        --------\n",
    "        n_nodes1 : 前の層のノード数\n",
    "        n_nodes2 : 現在の層のノード数\n",
    "        \"\"\"\n",
    "        self.sigma = 1 / np.sqrt(n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        \"\"\"\n",
    "        コンストラクタ\n",
    "        --------\n",
    "        n_nodes2 : 現在のノード数\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1,n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"He\"\"\"\n",
    "    def __init__(self,sigma):\n",
    "        _ = sigma\n",
    "        \n",
    "    def W(self,n_nodes1,n_nodes2):\n",
    "        \"\"\"\n",
    "        コンストラクタ\n",
    "        --------\n",
    "        n_nodes1 : 前の層のノード\n",
    "        n_nodes2 : 現在の層のノード\n",
    "        \"\"\"\n",
    "        self.sigma = np.sqrt(2/n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        \"\"\"\n",
    "        コンストラクタ\n",
    "        --------\n",
    "        n_nodes2 : 現在の層のノード数\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1,n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】最適化手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
    "\n",
    "\n",
    "まず、これまで使ってきたSGDを確認します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W′_i=W_i−αE\\frac{\\partial L}{\\partial W_i}\n",
    "$$\n",
    "$$\n",
    "B′_i=B_i−αE\\frac{\\partial L}{\\partial B_i}\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率（層ごとに変えることも可能だが、基本的にはすべて同じとする）\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_i}$ : $W_i$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B_i}$ : $B_i$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$E()$ : ミニバッチ方向にベクトルの平均を計算\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
    "\n",
    "\n",
    "更新された分だけその重みに対する学習率を徐々に下げていきます。イテレーションごとの勾配の二乗和 $H$ を保存しておき、その分だけ学習率を小さくします。\n",
    "\n",
    "\n",
    "学習率は重み一つひとつに対して異なることになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H′_i=H_i+E\\frac{\\partial L}{\\partial W_i}⊙ E\\frac{\\partial L}{\\partial W_i}\n",
    "$$\n",
    "$$\n",
    "W′_i=W_i−α\\frac{1}{√H′_i}E\\frac{\\partial L}{\\partial W_i}\n",
    "$$\n",
    "\n",
    "$H_i$ : i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
    "\n",
    "\n",
    "$H_i^{\\prime}$ : 更新した $H_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確率的勾配降下方の実装\n",
    "class AdaGrad:\n",
    "    \n",
    "    # インスタンス変数を定義\n",
    "    def __init__(self,lr):\n",
    "        self.lr = lr # 学習率\n",
    "        \n",
    "    # パラメータの更新メソッドを定義\n",
    "    def update(self,layer):\n",
    "        \"\"\"\n",
    "        コンストラクタ\n",
    "        --------\n",
    "        layer : layerインスタンス\n",
    "        \"\"\"\n",
    "        layer.HW += layer.dW * layer.dW\n",
    "        layer.HB += layer.dB * layer.dB\n",
    "        # パラメータごとに値を更新\n",
    "        delta = 1e-7#エラー対策\n",
    "        layer.W -= self.lr * layer.dW /(np.sqrt(layer.HW) + delta)/ len(layer.Z)\n",
    "        layer.B -= self.lr * layer.dB /(np.sqrt(layer.HB) + delta)/ len(layer.Z)\n",
    "        return layer\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】クラスの完成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        \"\"\"通常のコンストラクタと同様の働き\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 説明変数（画像の1次元データ）\n",
    "        y : 目的変数（ラベル）\n",
    "        batch_size : 必要なミニバッチのデータ数\n",
    "        seed : ランダムシード固定\n",
    "        \"\"\"\n",
    "        # ランダムシードの固定（学習ごとに同じ生成順）\n",
    "        np.random.seed(seed)\n",
    "        # バッチ数のメンバ変数\n",
    "        self.batch_size = batch_size\n",
    "        # データ全体の長さ分のインデックスをランダムに並べ替え\n",
    "        # np.random.permutation:配列をランダムに並べ替え\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        # 並べ替えたインデックスと同じ順番で説明変数と目的変数を並べ替え\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        # データ数をバッチ数で割って、何回呼び出せば、全データを学習したことになるかの判定\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # 何回目の呼び出しか\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # 全データを学習すればストップ\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        # 並び変えた_X,_yの何番目のインデックスを採用するか\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        # returnする前にカウンタに+1しておく\n",
    "        self._counter += 1\n",
    "        # 説明変数と目的変数を返す\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "\n",
    "    def __init__(self,batch_size=20,n_features=784,n_nodes1 =400,n_nodes2 = 200,n_output =10,lr =0.005,epoch=10,sigma=0.02,optimizer=SGD, initializer=HeInitializer,activater=ReLU,output_activater=Softmax,verbose=True):\n",
    "        \"\"\"コンストラクタ\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : バッチサイズ（default:20)\n",
    "        n_features : 説明変数の数（default:784)\n",
    "        n_nodes1 : 前の層のノード数（default:400)\n",
    "        n_nodes2 : 当該層のノード数（default:200)\n",
    "        n_output : 出力層のノード数（default:10)\n",
    "        sigma : 初期化時のパラメータ（default:0.02)\n",
    "        lr : 学習率（default:0.005)\n",
    "        verbose : 計算過程の出力（default:True)\n",
    "        epoch : 学習回数（default:10)\n",
    "        optimizer : 最適化手法（default:SGD)\n",
    "        initializer : 初期化方法（default:HeInitializer）\n",
    "        activater : 活性化関数（default:ReLU）\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.optimizer = optimizer\n",
    "        self.sigma = sigma\n",
    "        self.initializer = initializer\n",
    "        self.activater = activater\n",
    "        self.output_activater = output_activater\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self,X,y,X_val=None,y_val=None):\n",
    "        \"\"\"\n",
    "        学習\n",
    "        --------\n",
    "        X : 訓練データの説明変数\n",
    "        y : 訓練データの目的変数\n",
    "        X_val : 評価データの説明変数\n",
    "        y_val : 評価データの目的変数\n",
    "        \"\"\"\n",
    "        \n",
    "        # lossの記録用配列\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "        \n",
    "        # 最適化手法の初期化\n",
    "        optimizer = self.optimizer(self.lr)\n",
    "        \n",
    "        # 各層の初期化\n",
    "        self.FC1 = FC(self.n_features,self.n_nodes1,self.initializer(self.sigma),optimizer)\n",
    "        self.activation1 = self.activater()\n",
    "        self.FC2 = FC(self.n_nodes1,self.n_nodes2,self.initializer(self.sigma),optimizer)\n",
    "        self.activation2 = self.activater()\n",
    "        self.FC3 = FC(self.n_nodes2,self.n_output,self.initializer(self.sigma),optimizer)\n",
    "        self.activation3 = self.output_activater()\n",
    "        \n",
    "        # 学習回数分ループ\n",
    "        for i in range(self.epoch):\n",
    "            # ミニバッチイテレータ生成\n",
    "            get_mini_batch = GetMiniBatch(X,y,batch_size=self.batch_size,seed=i)\n",
    "            \n",
    "            # ミニバッチイテレータループ\n",
    "            for mini_X,mini_y in get_mini_batch:\n",
    "                ## 順伝播\n",
    "                \"\"\"１層目\"\"\"\n",
    "                A1 = self.FC1.forward(mini_X)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                \n",
    "                \"\"\"２層目\"\"\"\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                \n",
    "                \"\"\"３層目\"\"\"\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                \n",
    "                ## 逆伝播\n",
    "                dA3,loss = self.activation3.backward(Z3,mini_y)\n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1)\n",
    "                \n",
    "            # 通過出力\n",
    "            if self.verbose:\n",
    "                ## 順伝播\n",
    "                \"\"\"１層目\"\"\"\n",
    "                A1 = self.FC1.forward(X)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                \n",
    "                \"\"\"２層目\"\"\"\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                \n",
    "                \"\"\"３層目\"\"\"\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                \n",
    "                # 損失計算と記録\n",
    "                loss = self.activation3.backward(Z3,y)[1]\n",
    "                self.loss_train.append(loss)\n",
    "                print(i,loss)\n",
    "                \n",
    "                # 評価データを見る\n",
    "                if X_val is not None:\n",
    "                    ## 順伝播\n",
    "                    \"\"\"１層目\"\"\"\n",
    "                    A1 = self.FC1.forward(X_val)\n",
    "                    Z1 = self.activation1.forward(A1)\n",
    "                    \n",
    "                    \"\"\"２層目\"\"\"\n",
    "                    A2 = self.FC2.forward(Z1)\n",
    "                    Z2 = self.activation2.forward(A2)\n",
    "                    \n",
    "                    \"\"\"３層目\"\"\"\n",
    "                    A3 = self.FC3.forward(Z2)\n",
    "                    Z3 = self.activation3.forward(A3)\n",
    "                    \n",
    "                    # 損失計算と記録\n",
    "                    self.loss_val.append(self.activation3.backward(Z3,y_val)[1])\n",
    "                    \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        予測\n",
    "        --------\n",
    "        X : 入力配列\n",
    "        \"\"\"\n",
    "        # 順伝播\n",
    "        \"\"\"1層目\"\"\"\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "\n",
    "        \"\"\"２層目\"\"\"\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "\n",
    "        \"\"\"３層目\"\"\"\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "\n",
    "        # 最も大きいインデックスを採用\n",
    "        return np.argmax(Z3,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題9】学習と推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 画像データ→行データ\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# 分割(訓練データ・評価データ\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "# one-hotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_val[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2881794350390807\n",
      "1 0.7971158909095009\n",
      "2 0.5991456777002023\n",
      "3 0.4999276653667498\n",
      "4 0.44215152877821207\n",
      "5 0.4003154254156251\n",
      "6 0.36395891650811785\n",
      "7 0.3412018818521649\n",
      "8 0.3186649025267526\n",
      "9 0.3019268084831267\n",
      "10 0.2903831637548931\n",
      "11 0.27427279538313903\n",
      "12 0.262736069310421\n",
      "13 0.25158179908151823\n",
      "14 0.24288484045389191\n",
      "15 0.23129781494829604\n",
      "16 0.2258765613126353\n",
      "17 0.21725473787762103\n",
      "18 0.209149770916316\n",
      "19 0.2029759653108972\n",
      "20 0.1948866461423655\n",
      "21 0.19256616144147914\n",
      "22 0.18234379396945238\n",
      "23 0.17981474183847138\n",
      "24 0.17109258092914692\n",
      "25 0.1673770312625774\n",
      "26 0.1626155313204266\n",
      "27 0.1579622837426295\n",
      "28 0.1512574476825076\n",
      "29 0.14843367423128462\n",
      "30 0.14256800420172572\n",
      "31 0.13916564503969892\n",
      "32 0.1349197030567693\n",
      "33 0.13332751170576024\n",
      "34 0.12854291089736175\n",
      "35 0.12424178814213561\n",
      "36 0.12038339442348772\n",
      "37 0.11796839110368362\n",
      "38 0.1143324403070811\n",
      "39 0.11116668999984089\n",
      "40 0.10825010101258513\n",
      "41 0.10577920884100261\n",
      "42 0.10256357047208543\n",
      "43 0.10079485482799802\n",
      "44 0.09743294623886854\n",
      "45 0.09553813895293448\n",
      "46 0.09245067509714212\n",
      "47 0.08903134469835644\n",
      "48 0.08680603234224572\n",
      "49 0.0846814081555115\n",
      "50 0.08336922089984028\n",
      "51 0.08137554257465508\n",
      "52 0.07872648567158197\n",
      "53 0.07621638202855976\n",
      "54 0.07567194769483761\n",
      "55 0.07404913964413193\n",
      "56 0.07130503142339407\n",
      "57 0.06887571638508035\n",
      "58 0.0669101083883617\n",
      "59 0.06646333323149511\n",
      "60 0.06408641620335004\n",
      "61 0.061901065594586316\n",
      "62 0.06047297194589216\n",
      "63 0.05879121050943839\n",
      "64 0.058310904062000166\n",
      "65 0.05595892617357288\n",
      "66 0.0542847114910632\n",
      "67 0.053346966407961036\n",
      "68 0.05225859632130365\n",
      "69 0.05061118733862155\n",
      "70 0.049240671944484934\n",
      "71 0.048278930291948755\n",
      "72 0.04752153662422417\n",
      "73 0.04580178718221474\n",
      "74 0.045380423623567805\n",
      "75 0.04379644767036021\n",
      "76 0.04281950391907014\n",
      "77 0.04175476984595271\n",
      "78 0.04084167604053683\n",
      "79 0.03998798882310546\n",
      "80 0.03880757659850381\n",
      "81 0.03876609414355694\n",
      "82 0.037217866186909226\n",
      "83 0.036545954372947054\n",
      "84 0.03593305715029133\n",
      "85 0.03483330161978869\n",
      "86 0.0344091145501413\n",
      "87 0.03368854418771779\n",
      "88 0.032886602858075605\n",
      "89 0.03198251378225195\n",
      "90 0.03159525822204505\n",
      "91 0.03062225458621122\n",
      "92 0.030536126510941987\n",
      "93 0.029808833490762923\n",
      "94 0.029140097870354445\n",
      "95 0.02858793871410314\n",
      "96 0.027957203733844752\n",
      "97 0.027280225293952083\n",
      "98 0.026745670789779866\n",
      "99 0.026430925615002085\n"
     ]
    }
   ],
   "source": [
    "dnn = ScratchDeepNeuralNetrowkClassifier(epoch=100) \n",
    "\n",
    "dnn.fit(X_train[:4000], y_train_one_hot[:4000], X_val[:2000], y_test_one_hot[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9274166666666667"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = dnn.predict(X_val)\n",
    "accuracy_score(y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(1, dnn.epoch+1)), dnn.loss_train, label='train')\n",
    "plt.plot(list(range(1, dnn.epoch+1)), dnn.loss_val, label='test')\n",
    "plt.legend()\n",
    "plt.xticks(list(range(1, dnn.epoch+1)));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
